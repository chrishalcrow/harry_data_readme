[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Experiment summary",
    "section": "",
    "text": "This experiment is designed to find out stuff about memory."
  },
  {
    "objectID": "index.html#session-types",
    "href": "index.html#session-types",
    "title": "Experiment summary",
    "section": "Session Types",
    "text": "Session Types\nSeveral different experiment types were considered. Each experiment contains sessions. Each session has an abbreviated label such as OF1. Knowing which label you are looking for will help when accessing the data.\nHere, we summarise each experiment type and the sessions within each experiment.\n\n\nOpen field -&gt; VR -&gt; Open field\n\nOF1. The mouse explores the open field arena for 20 minutes.\nVR. The mouse is transferred to a head fixed VR enviroment doing a memory task. Description of memory task.\nOF2. The mouse is returned to the open field arena for another 20 minutes of free exploration.\n\n\n\nOpen field -&gt; VR multi context -&gt; Open field\n\n\n\n\nVisual Coding\n\n\n\n\nVisual Sequences\n\n\n\n\nVisual multi-sequences"
  },
  {
    "objectID": "index.html#data-and-pipeline-overview",
    "href": "index.html#data-and-pipeline-overview",
    "title": "Experiment summary",
    "section": "Data and pipeline overview",
    "text": "Data and pipeline overview\nOur basic pipeline overview can be found below.\n\n\n\nPipeline overview\n\n\nEvery session contains an output from Bonsai or Blender and Video. These capture the animal behaviour data, as well as a light pulse signal used for synchronisation. Most sessions include ephys data, which capture neural behaviour. Metadata about the ephys recording is also saved, such as which recording configuration was used is also saved.\nWe expect users will be most interested in raw data (in red) or the outputs for analysis (in teal). Find out how to access the Raw data and Derived data below. We explain how each script was implemented in the Code Protocols section."
  },
  {
    "objectID": "index.html#folder-structure",
    "href": "index.html#folder-structure",
    "title": "Experiment summary",
    "section": "Folder Structure",
    "text": "Folder Structure\n\nRaw data\nThe raw data is organised as follows:\ndata_folder/\n    session_folder/\n        M{mouse}_D{day}_datetime_{session_abbreviation}/\n            params.yaml  # metadata about the experiment\n            {video_file}.avi  # video of behaviour\n            {behaviour_files}.csv  # behavioural output from Bonsai/Blender\n            Record Node 102/  # Ephys data\n                experiment1/\n                    recording1/\n                        continous/\n                        events/ (not used)\n                        spikes/ (not used)\n                        structure.oebin\n            \nwhere datetime is the time the session began.\nFor example, the VR data for mouse 25, day 20 in stored in\n\"data_folder/vr/M25_D20_2024-11-08_11-52-50_VR1\"\nFind out more about the data below.\n\n\nDerived data\nEach mouse day (usually) corresponds to one “experiment”, which might contain severl “sessions”. To reflect this, the derived data is organised as follows:\nderivatives_folder/\n    M{mouse}/\n        D{day}/\n            full/\n            {session_type_1}/\n            {session_type_2}/\n            ...\nThe information in full is shared between all sessions in the experiment (e.g. if all the data is sorted together, the sorted data is stored here) while information unique to each session is stored in the session_type_n folder. This folder can contain many pieces of data. Each is described in the DataTypes section below.\n\n\nAn example\n\nOn day 20, mouse 25 took part in the OF1-&gt;VR-&gt;OF2 experiment. Its derivatives folder looks like\nM25/D20/\n    full/\n        kilosort4/\n            kilosort4_report/\n            kilosort4_sa/\n        rec_samples.json\n    of1/\n        dlc/\n        Figures/\n        kilosort4/\n            spikes.pkl\n        position_data.csv\n        theta_phase.pkl\n    vr/\n        Figures/\n        kilosort4/\n            spikes.pkl\n        licks/\n            lick_data.csv\n        pupil_dilation/\n            pupil_dilation.csv\n        position_data.csv\n        theta_phase.pkl\n    of2/ \n        dlc/\n        Figures/\n        kilosort4/\n            spikes.pkl\n        position_data.csv\n        theta_phase.pkl\nAnd more! Importantly, the data which depends on the sorting algorithm used are contained in a folder named for that algorithm. E.g. the spikes.pkl file contains the spike times generated by the kilosort4 algorithm so belong to this folder."
  },
  {
    "objectID": "index.html#accessing-raw-data",
    "href": "index.html#accessing-raw-data",
    "title": "Experiment summary",
    "section": "Accessing raw data",
    "text": "Accessing raw data\nHere we summarise the most important data for collaborators to access.\n\nRaw Ephys recordings\nStored at\ndata_folder/Cohort_folder/session_folder/M1_D1_datetime_{session}/\nOpenphys files. Roughly a large binary file with some metadata.\nThese can be read using spikeinterface e.g.\nimport spikeinterface.full as si\npath_to_recording = \"data_folder/of/M25_D20_2024-11-08_11-25-37_OF1/\"\nrecording = si.read_openephys(path_to_recording)\n\n\nVideo files\nStored at\ndata_folder/Cohort_folder/session_folder/M1_D1_datetime_{session}/{video_name}.avi\nAvi files.\nOpen Field. Video M{mouse}_D{day}_OF1.avi gives a top-down view of the open field arena. Used to determine mouse position. Frame rate is 15 frames per second.\nVR. Video M{mouse}_D{day}_side_capture*.avi gives side view of mouse while running. Used to determind Tongue position, pupil dilation and leg position. Frame rate is 60(??) frames per second."
  },
  {
    "objectID": "index.html#accessing-derived-data",
    "href": "index.html#accessing-derived-data",
    "title": "Experiment summary",
    "section": "Accessing derived data",
    "text": "Accessing derived data\n\nSortingAnalyzer\nStored at\nderivatives_folder/M{mouse}/D{day}/full/{sorter_name}_{sorting_protocol}/{sorter_name}_{sorting_protocol}_sa/\nA spikeinterface SortingAnalyzer object, containing spike times and dervied information such as unit templates, spike locations etc. The analyzer depends on the sorter used. We label each sorting protocol by a number. The details of the protocols can be found in the Protocols section.\nCan be read using spikeinterface\nimport spikeinterface.full as si\nsa_path = \"derivatives/M25/D20/full/kilosort4_3/kilosort4_3_sa\"\nsorting_analyzer = si.read_sorting_analyzer(sa_path)\nRead more about SortingAnalyzers here, here and here (video).\n\n\nSpike Trains\nStored at\nderivatives_folder/M{mouse}/D{day}/session_folder/sorter_folder/spikes.pkl\nA pickle file containing spike trains for each sorted unit, perhaps also with other information. When unpickled, the output is a pandas DataFrame. The unit ids of each unit are contained in the ‘cluster_id’ column and spike trains for each unit are contained in the firing_times column. The cluster_ids match those found in the SortingAnalyzer.\nCan be read using pandas.\nimport pandas as pd\nspike_data = pd.read_pickle(\"derivatives/M25/D20/of1/kilosort/spikes.pkl\")\nall_spike_trains = spike_data[['cluster_id', 'firing_times]]\nspike_train_for_unit_five = all_spike_trains[5]\n\n\nOpen Field Position Data (OF1 + OF2)\nStored at\nderivatives_folder/M{mouse}/D{day}/of{n}/position_data.csv\nA csv file containing position and light pulse data (used for syncing). Position computed using deeplabcut. Important columns are: position_x, position_y and synced_time.\nCan be read using e.g. pandas.\nimport pandas as pd\nposition_data = pd.read_csv(\"derivatives/M25/D20/of1/position_data.csv\")\nposition_x = position_data['position_x']\n\n\nTongue position\nStored at\nderivatives_folder/M{mouse}/D{day}/vr/licks/\nA csv file containing the position of the “tongue”, extracted using deeplabcut. Our strategy was to teach deeplabcut to use the lip as the tip of the tongue when the mouth is closed. Then when the tongue comes out, use the tip of the tongue. Hence we expect the position data to be clustered into two rough clusters: near the lips when the mouth is closed and near the reward spout when the mouth is open. High velocity should correspond to licks.\nThe rawest data from deeplabcut is stored in the file ending 200.csv while a median filtered verison is stored in the file ending 200_filtered.csv.\nCan be read using e.g. pandas. The dlc format has an abnormal header, meaning we need to pass some extra arguments to read_csv.\nimport pandas as pd\ntongue_data = pd.read_csv(\"derivatives/M25/D20/vr/M25_D20_si..._200.csv\", header=[1, 2], index_col=0)\ntongue_position_x = position_data['x']\n\n\nChannel locations in CCF coordinates\nStored at\nderivatives_folder/labels/all_ccf_coords_per_channel.csv\nA .csv file containing the locations of each contact_id of each probe in CCF coordinates. This can be used in conjunction with the raw recording or sorting analyzer to do lots of stuff.\nWhen thinking about clusters, we usually think in terms of channel_ids but their labelling is actually related to how the raw binary file is stored and don’t give any information about the electrode location. That information is stored in the contact_id property of the probe. This makes the code a little messy, since we need a map from channel_ids to contact_ids and another from contact_ids to CCF_coords.\nHere’s how to access this stuff from the raw recording\nimport spikeinterface.full as si\n\nimport pandas as pd\nimport numpy as np\nall_brain_locations = pd.read_csv(\".../all_ccf_coords_per_channel.csv\")\nthis_mouse_brain_locations = all_brain_locations.query('mouse == 25')\n\n# make a dict of contact_ids -&gt; CCF brain coordinates\ncontact_ids_to_CCF = { \n    contact['contact_id']: np.array([contact['z_CCF'], contact['y_CCF'], contact['x_CCF']]) \n    for _, contact in this_mouse_brain_locations.iterrows()\n}\n\n# now make a dict of channel ids -&gt; contact_ids\nrecording = si.read_openephys(\".../M25_D20_2024-11-08_11-25-37_OF1/\")\ncontact_ids = recording.get_probe().contact_ids\nchannel_ids_to_contact_ids = dict(zip(recording.channel_ids, contact_ids[recording.ids_to_indices()]))\n\n# you can then use these dicts to get the brain coordinate for a given channel id\nbrain_coord_for_CH100 = contact_ids_to_CCF[channel_ids_to_contact_ids['CH100']]\nOr you might want to use the CCF coordinates alongside a SortingAnalyzer, e.g. to find out where units are\nsorting_analyzer = si.load_sorting_analyzer(\".../kilosort4_3_sa\")\ncontact_ids = sorting_analyzer.get_probe().contact_ids\n\n# find the extremum channels, which is a dict from unit_id to channel_id\nextremum_channels = si.get_template_extremum_channel(sorting_analyzer)\n\nbrain_coord_for_unit_10 = contact_ids_to_CCF[channel_ids_to_contact_ids[extremum_channels[10]]]"
  },
  {
    "objectID": "index.html#code-protocols",
    "href": "index.html#code-protocols",
    "title": "Experiment summary",
    "section": "Code Protocols",
    "text": "Code Protocols\nHello!"
  },
  {
    "objectID": "index.html#experimental-protocols",
    "href": "index.html#experimental-protocols",
    "title": "Experiment summary",
    "section": "Experimental Protocols",
    "text": "Experimental Protocols\nadd links to nolanlab wiki for surgery protocol for neuropixel implantation add links to nolanlab wiki for experimental protocol including behaviour, water dep etc add links to nolanlab wiki for extracting coordniates from DiI tracks using Probe-TRACK and custom scripts add links to nolanlab wiki for extracting features from video including licks, pupil dilation, steps and pose anything else?"
  },
  {
    "objectID": "index.html#feature-extraction-from-video-data",
    "href": "index.html#feature-extraction-from-video-data",
    "title": "Experiment summary",
    "section": "Feature Extraction from video data",
    "text": "Feature Extraction from video data\nHow do we extract behavioural variables from video footage? Deeplabcut! We manually labelled the pixel location for features of interest from raw video data. We then trained bespoke deeplabcut models to infer pixel positions for each video in the dataset.\nOpen field videos captured the mouse’s movement in the square arena with a birds eye view. We estimated the mouse’s pose using five features including head, shoulders, middle, tail start and tail end. Pixel coordinates coordinates were transformed and scaled to match the 1 m x 1 m dimensions of the open arena. To estimate head direction, we took the vector between the head and middle features and calculated an angle relative to north.\nAdd image or gif?\nVirtual reality videos captured a side view of the mouse while head-restraint on a cylindrical treadmill. We estimated the mouse’s pupil diameter using eight features including eye north, eye north-east, eye-east etc. Pupil diamter was defined as the average pixel distance between opposite features such as north vs south, north-east vs south-west etc. Units for pupil diameter are arbitrarily defined as we were only concerned with relative changes.\nadd image or gif of pupil models\nWe estimated when the mouse’s was engaged in licking using a single feature marking the position of the tongue. When the tongue was out of sight (in the mouse’s mouth), we marked the tongue’s position as the bottom of the mouth. Plotting the tongues position across a session typically produced a point cloud with two clearly defined clusters. To label positions attributable to lick events, we manually drew around the lower positioned cluster. Finally we visually verified this method by creating gif snippets for each session and accessing whether licks were labelled accurately.\n  \nadd image or gif of lick models"
  },
  {
    "objectID": "index.html#faq",
    "href": "index.html#faq",
    "title": "Experiment summary",
    "section": "FAQ",
    "text": "FAQ\nHow was the experiment conducted on a typical day?\nExperimental days involved recording from mice in the open arena and then in the virtual location memory task and then once again in the open arena. Mice were collected from the holding room 30 - 60 minutes before recording, were handled for 5 - 10 minutes, weighed and placed for 10 - 20 minutes in a cage containing objects and a running wheel. Between recording sessions mice were placed back in the object-filled playground for 10 - 20 minutes. The open arena consisted of a metal box with a square floor area, removable metal walls, metal frame (Frame parts from Kanya UK, C01-1, C20-10, A33-12, B49-75, B48-75, A39-31, ALU3), and an A4-sized cue card in the middle of one of the metal walls. For the open field exploration session, mice were placed in the open arena while tethered via an ultrathin Neuropixel aquisition cable No Commutator was used. In a small number of sessions, tangling of the Neuropixel cable caused the cable to fall in front of the mouse. To stop mice from knawing on the cable, Mice were quickly untangled by the experimentor. For the location memory task water-restricted mice were trained to obtain rewards at a location on the virtual linear track. Mice were head-fixed using a RIVETS clamp (Ronal Tool Company, Inc) and ran on a cylindrical treadmill fitted with a rotary encoder (Pewatron). Virtual tracks, generated using Blender3D (blender.com) had length 200 cm, with a 60 cm track zone, a 20 cm reward zone, a second 60 cm track zone and a 60 cm black box to separate successive trials. The distance visible ahead of the mouse was 50 cm. The reward zone was either marked by distinct vertical green and black bars on beaconed trials, or was not marked by a visual cue at all on non-beaconed. A feeding tube placed in front of the animal dispensed 10 % sucrose water rewards (5 - 10 5l per reward) if the mouse stopped in the reward zone. A stop was registered in Blender3D if the speed of the mouse dropped below 3 cm/s. Speed was calculated on a rolling basis from the previous 100 ms at a rate of 60 Hz. Trials were delivered in a random fashion with a equal probability on any given trial of being beaconed or non-beaconed. Mice were trained to an expert level in the location memory task before being implanted with Neuropixel 2.0 probes and undergoing the three session (open arena/location memory task/open arena) For more details, see Clark and Nolan 2024, https://doi.org/10.7554/eLife.89356.2\nHow was data collected?\nElectrophysiological signals were acquired using a Neuropixel 2.0 headstage connected via an Neuropixel aquisition cable attached to an IMEC-National Instruments aquisition system (recommended hardware as of July 2024). For the location memory task, positional and trial information was saved in Blender3D at 60 Hz and time sync with TTL pulses delivered concurrently to the Neuropixel aquisition system and the Blender3D computer via an arduino Due. In the open arena, motion and head-direction tracking used a camera (Logitech B525, 1280 x 720 pixels Webcam, RS components 795-0876) attached to the celing of the frame. A custom bonsai tracking script picked up the TTL pulses delivered to an LED in sight of the tracking camera and out of sight of the freely exploring mouse. Body and head direction tracking was completed post-recording using DeepLabCut.\nWhat was the surgery schedule of the experiment?\nMice underwent two seperate surgeries under anaesthesia, the first being a headpost attachment surgery followed by a Neuropixel 2.0 implantation mounted on a Apollo drive resuable 3D printed body. All surgeries were performed by Harry Clark\nWhat drugs were used in the experiment?\nIsoflurane used during surgery. Vetergesic jelly given post surgery. Carprofen and Buprenorphine were given subcutaneously at the recommended dosage post surgery\nHow can I reproduce the stereotaxic surgery?\nAdd this\nWhere can I find files for 3D printing the reusable 3D printed components?\nAdd this Include rivets custom designs\nWhere can I find the used NP2 probes for use in future experiments?\nThere are currently 4 x NP2 4-shank probes ready to be used for implantation. These can be found within the 2nd floor wet lab room (add room number) in the drive building area. A box labelled with “Neuropixel 2.0 4 shank probes, Harry Clark” can be found on the shelf above the drive building bench in the drive staging area (marked by yellow tape). (This information was accurate as of 04/03/2025). These drives are super-glued to an Apollo drive shuttle and thus can only be used for further experimentation with Apollo drive compatible components.\nStill missing a vital piece of information?\nEmail harrydclark91@gmail.com for further clarification so we can add the relevant information to this document."
  }
]