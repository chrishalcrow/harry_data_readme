---
title: Experiment summary
output: html_document
theme: spacelab
format:
  html:
    toc: true
    toc-location: left
---

## Overview

TEST. This experiment is designed to find out stuff about memory. 

## Experimental overview

We stereotaxically targeted the medial entorhinal cortex of 7 males and 1 female C57BL/6J mice with Neuropixel 2.0 (NP2; 4 shank) probes. Prior to implantation of the NP2 probes, mice were trained in the VR linear track experiment to an expert level in both beaconed and non-beaconed trial variants (see Tennant et al., 2018; 10.1016/j.celrep.2018.01.005).

Changes to the task from Tennant et al., 2018, Tennant et al., 2022 and Clark and Nolan, 2024 (T18, T22 and C24 respectively):

- Water deprivation was used instead of food deprivation therefore 10% sucrose solution was used as reward instead of soy milk. The volume of sucrose solution varied between 5-10 uL per reward dispensed.
- A speed of 3 cm/s was used as the stop threshold, compared to 0.7 cm/s and 4.7 cm/s used in T18 and T22. 
- Probe trials (non-beaconed trials without reward) were not recorded in this experiment. 
- Upon teleportation back to the start of the track, trial type was randomised with equal weighting to beaconed and non-beaconed trials, this produced an equal number of beaconed and non-beaconed trials compared to the repeated block structures used in T18 and T22. 
- Mice were only implanted once they reached an expert level in the task.
- Electrophysiology was acquired using a National Instruments/IMEC acquisition system and recorded using Open Ephys software using the Neuropixel acquisition external plugin.
- Behavioural variables were stored and saved directly from Blender3D, and time synced retrospectively using aperiodic TTL pulses sent directly to both the Neuropixel acquisition system and Blender3D via a custom Arduino Due.


## Experiment and session Types

Several different experiment types were considered. Each experiment contains sessions. Each session has an abbreviated label such as OF1. Knowing which label you are looking for will help when accessing the data.

Here, we summarise each experiment type and the sessions within each experiment.

<details>
<summary>Open field -> VR -> Open field</summary>

**OF1**. The mouse explores the open field arena for 20 minutes.

**VR**. The mouse is transferred to a head fixed VR enviroment doing a memory task. Description of memory task. 

**OF2**. The mouse is returned to the open field arena for another 20 minutes of free exploration.
</details>

<details>
<summary>Open field -> VR multi context -> Open field</summary>
</details>

<details>
<summary>Visual Coding</summary>

</details>

<details>
<summary>Visual Sequences</summary>

</details>

<details>
<summary>Visual multi-sequences</summary>

</details>

## Folder Structure

### Raw data

The raw data is organised as follows:

``` python
data_folder/
    session_folder/
        M{mouse}_D{day}_datetime_{session_abbreviation}/
            Record Node 102/
                experiment1/
                    recording1/
                        continous/
                        events/ (not used)
                        spikes/ (not used)
                        structure.oebin
            —VIDEOFILE - This is a .avi file that either captures pupil and licks or Birds Eye view tracking. TTL pulses are visible in the video and are typically computed into bonsai output .csv files to quantify the sum of a bounding box around a LED with a signal consistent with the timing of the TTL pulses. TTL pulses can be recomputed from video files if needed. 
            —BEHAVIOURFILE - This is a .csv file either a blender output file (contains TTL pulse array per sample) or a bonsai output file containing position information (contains TTL pulses per sample)
            —params.yaml - This file contains specific information to the recording that needs to be given as extra information to make sense of the recording, such as the stopping threshold used in blender for VR.
```

where datetime is the time the session began.

For example, the VR data for mouse 25, day 20 in stored in

```
"data_folder/vr/M25_D20_2024-11-08_11-52-50_VR1"
```

Within each folder is the raw data extracted during the experiment. Depending on the session, this might contain electrophysiology data, behavioural data or both. Find out more in the Datatypes section below.



### Derived data

The derived data is organised as follows:

``` python
derivatives_folder/
    M{mouse}/
        D{day}/
            full/
            {session_type_1}/
            {session_type_2}/
            ...
```

So that each `experiment` is grouped together. The information in `full` is shared between all sessions in the experiment (e.g. if all the data is sorted together, the sorted data is stored here) while information unique to each session is stored in the `session_type_n` folder. This folder can contain *many* pieces of data. Each is described in the DataTypes section below.

<details>
<summary>An example</summary>
On day 20, mouse 25 took part in the OF1->VR->OF2 experiment. Its derivatives folder looks like

```
M25/D20/
    full/
        kilosort4/
            kilosort4_report/
            kilosort4_sa/
        rec_samples.json
    of1/
        dlc/
        Figures/
        kilosort4/
            spikes.pkl
        position_data.csv
        theta_phase.pkl
    vr/
        Figures/
        kilosort4/
            spikes.pkl
        position_data.csv
        theta_phase.pkl
    of2/ 
        dlc/
        Figures/
        kilosort4/
            spikes.pkl
        position_data.csv
        theta_phase.pkl
```

And more! Each of the folders contain more data and outputs. Importantly, the data which depends on the sorting algorithm used are contained in a folder named for that algorithm. E.g. the spikes.pkl file contains the spike times generated by the kilosort4 algorithm so belong to this folder. The data contained here is described in detail in the Datatypes section below.

</details>

## Accessing data

Here we summarise the most important data for collaborators to access.

### Raw Ephys recordings

Stored at
```
data_folder/Cohort_folder/session_folder/M1_D1_datetime_{session}/
```

Openphys files. Roughly a large binary file with some metadata.

These can be read using `spikeinterface` e.g.

``` python
import spikeinterface.full as si
path_to_recording = "Cohort12_august2024/of/M25_D20_2024-11-08_11-25-37_OF1/"
recording = si.read_openephys(path_to_recording)
```

### SortingAnalyzer

Stored at
```
derivatives_folder/M{mouse}/D{day}/full/{sorter_name}_{sorting_protocol}/{sorter_name}_{sorting_protocol}_sa/
```

A `spikeinterface` `SortingAnalyzer` object, containing spike times and dervied information such as unit templates, spike locations etc. The analyzer depends on the sorter used. We label each sorting protocol by a number. The details of the protocols can be found in the [Protocols](#protocols) section.

Can be read using `spikeinterface`
``` python
import spikeinterface.full as si
sa_path = "derivatives/M25/D20/full/kilosort4_3/kilosort4_3_sa"
sorting_analyzer = si.read_sorting_analyzer(sa_path)
```

Read more about `SortingAnalyzer`s here: .

### Spike Trains

Stored at
```
derivatives_folder/M{mouse}/D{day}/session_folder/sorter_folder/spikes.pkl
```

A pickle file containing spike trains for each sorted unit, perhaps also with other information. When unpickled, the output is a pandas DataFrame. The unit ids of each unit are contained in the 'cluster_id' column and spike trains for each unit are contained in the `firing_times` column. The `cluster_id`s match those found in the [SortingAnalyzer](#SortingAnalyzer).

Can be read using `pandas`.

``` python
import pandas as pd
spike_data = pd.read_pickle("derivatives/M25/D20/of1/kilosort/spikes.pkl")
all_spike_trains = spike_data[['cluster_id', 'firing_times]]
spike_train_for_unit_five = all_spike_trains[5]
```



### Open Field Position Data (OF1 + OF2)

Stored at
```
derivatives_folder/M{mouse}/D{day}/of{n}/position_data.csv
```

A csv file containing position and light pulse data (used for syncing). Position computed using deeplabcut. Important columns are: position_x, position_y and synced_time.

Can be read using e.g. `pandas`.

``` python
import pandas as pd
position_data = pd.read_csv("derivatives/M25/D20/of1/position_data.csv")
position_x = position_data['position_x']
```

### Tongue position

Stored at 
```
derivatives_folder/M{mouse}/D{day}/vr/licks/
```

A csv file containing the position of the "tongue", extracted using deeplabcut. Our strategy was to teach deeplabcut to use the lip as the tip of the tongue when the mouth is closed. Then when the tongue comes out, use the tip of the tongue. Hence we expect the position data to be clustered into two rough clusters: near the lips when the mouth is closed and near the reward spout when the mouth is open. High velocity should correspond to licks.

The rawest data from deeplabcut is stored in the file ending `200.csv` while a median filtered verison is stored in the file ending `200_filtered.csv`.

Can be read using e.g. `pandas`. The dlc format has an abnormal header, meaning we need to pass some extra arguments to `read_csv`.

``` python
import pandas as pd
tongue_data = pd.read_csv("derivatives/M25/D20/vr/M25_D20_si..._200.csv", header=[1, 2], index_col=0)
tongue_position_x = position_data['x']
```


### Channel locations in CCF coordinates

Stored at
```
derivatives_folder/labels/all_ccf_coords_per_channel.csv
```

A `.csv` file containing the locations of each `contact_id` of each probe in CCF coordinates. This can be used in conjunction with the raw recording or sorting analyzer to do lots of stuff.

When thinking about clusters, we usually think in terms of `channel_ids` but their labelling is actually related to how the raw binary file is stored and don't give any information about the electrode location. That information is stored
in the `contact_id` property of the probe. This makes the code a little messy, since we need a map from `channel_ids` to `contact_ids` and another from `contact_ids` to `CCF_coords`.

Here's how to access this stuff from the raw recording

``` python
import spikeinterface.full as si

import pandas as pd
import numpy as np
all_brain_locations = pd.read_csv(".../all_ccf_coords_per_channel.csv")
this_mouse_brain_locations = all_brain_locations.query('mouse == 25')

# make a dict of contact_ids -> CCF brain coordinates
contact_ids_to_CCF = { 
    contact['contact_id']: np.array([contact['z_CCF'], contact['y_CCF'], contact['x_CCF']]) 
    for _, contact in this_mouse_brain_locations.iterrows()
}

# now make a dict of channel ids -> contact_ids
recording = si.read_openephys(".../M25_D20_2024-11-08_11-25-37_OF1/")
contact_ids = recording.get_probe().contact_ids
channel_ids_to_contact_ids = dict(zip(recording.channel_ids, contact_ids[recording.ids_to_indices()]))

# you can then use these dicts to get the brain coordinate for a given channel id
brain_coord_for_CH100 = contact_ids_to_CCF[channel_ids_to_contact_ids['CH100']]
```

Or you might want to use the CCF coordinates alongside a SortingAnalyzer, e.g. to find out where units are

``` python
sorting_analyzer = si.load_sorting_analyzer(".../kilosort4_3_sa")
contact_ids = sorting_analyzer.get_probe().contact_ids

# find the extremum channels, which is a dict from unit_id to channel_id
extremum_channels = si.get_template_extremum_channel(sorting_analyzer)

brain_coord_for_unit_10 = contact_ids_to_CCF[channel_ids_to_contact_ids[extremum_channels[10]]]
```

## Technical details

### Protocols

add links to nolanlab wiki for surgery protocol for neuropixel implantation
add links to nolanlab wiki for experimental protocol including behaviour, water dep etc
add links to nolanlab wiki for extracting coordniates from DiI tracks using Probe-TRACK and custom scripts
add links to nolanlab wiki for extracting features from video including licks, pupil dilation, steps and pose
anything else?

### FAQ 

**How was the experiment conducted on a typical day?**

Experimental days involved recording from mice in the open arena and then in the virtual location memory task and then once again in the open arena. Mice were collected from the holding room 30 - 60 minutes before recording, were handled for 5 - 10 minutes, weighed and placed for 10 - 20 minutes in a cage containing objects and a running wheel. Between recording sessions mice were placed back in the object-filled playground for 10 - 20 minutes. The open arena consisted of a metal box with a square floor area, removable metal walls, metal frame (Frame parts from Kanya UK, C01-1, C20-10, A33-12, B49-75, B48-75, A39-31, ALU3), and an A4-sized cue card in the middle of one of the metal walls. For the open field exploration session, mice were placed in the open arena while tethered via an ultrathin Neuropixel aquisition cable No Commutator was used. In a small number of sessions, tangling of the Neuropixel cable caused the cable to fall in front of the mouse. To stop mice from knawing on the cable, Mice were quickly untangled by the experimentor. For the location memory task water-restricted mice were trained to obtain rewards at a location on the virtual linear track. Mice were head-fixed using a RIVETS clamp (Ronal Tool Company, Inc) and ran on a cylindrical treadmill fitted with a rotary encoder (Pewatron). Virtual tracks, generated using Blender3D (blender.com) had length 200 cm, with a 60 cm track zone, a 20 cm reward zone, a second 60 cm track zone and a 60 cm black box to separate successive trials. The distance visible ahead of the mouse was 50 cm. The reward zone was either marked by distinct vertical green and black bars on beaconed trials, or was not marked by a visual cue at all on non-beaconed. A feeding tube placed in front of the animal dispensed 10 % sucrose water rewards (5 - 10 \xB5l per reward) if the mouse stopped in the reward zone. A stop was registered in Blender3D if the speed of the mouse dropped below 3 cm/s. Speed was calculated on a rolling basis from the previous 100 ms at a rate of 60 Hz. Trials were delivered in a random fashion with a equal probability on any given trial of being beaconed or non-beaconed. Mice were trained to an expert level in the location memory task before being implanted with Neuropixel 2.0 probes and undergoing the three session (open arena/location memory task/open arena) For more details, see Clark and Nolan 2024, https://doi.org/10.7554/eLife.89356.2

**How was data collected?**

Electrophysiological signals were acquired using a Neuropixel 2.0 headstage connected via an Neuropixel aquisition cable attached to an IMEC-National Instruments aquisition system (recommended hardware as of July 2024). For the location memory task, positional and trial information was saved in Blender3D at 60 Hz and time sync with TTL pulses delivered concurrently to the Neuropixel aquisition system and the Blender3D computer via an arduino Due. In the open arena, motion and head-direction tracking used a camera (Logitech B525, 1280 x 720 pixels Webcam, RS components 795-0876) attached to the celing of the frame. A custom bonsai tracking script picked up the TTL pulses delivered to an LED in sight of the tracking camera and out of sight of the freely exploring mouse. Body and head direction tracking was completed post-recording using DeepLabCut.    

**What was the surgery schedule of the experiment?**

Mice underwent two seperate surgeries under anaesthesia, the first being a headpost attachment surgery followed by a Neuropixel 2.0 implantation mounted on a Apollo drive resuable 3D printed body. All surgeries were performed by Harry Clark

**What drugs were used in the experiment?**

Isoflurane used during surgery. Vetergesic jelly given post surgery. Carprofen and Buprenorphine were given subcutaneously at the recommended dosage post surgery

**How can I reproduce the stereotaxic surgery?**

Add this 

**Where can I find files for 3D printing the reusable 3D printed components?**

Add this
Include rivets custom designs

**Where can I find the used NP2 probes for use in future experiments?**

There are currently 4 x NP2 4-shank probes ready to be used for implantation. These can be found within the 2nd floor wet lab room (add room number) in the drive building area. 
A box labelled with “Neuropixel 2.0 4 shank probes, Harry Clark” can be found on the shelf above the drive building bench in the drive staging area (marked by yellow tape). 
(This information was accurate as of 04/03/2025). These drives are super-glued to an Apollo drive shuttle and thus can only be used for further experimentation with Apollo drive compatible components.

**Still missing a vital piece of information?**

Email harrydclark91@gmail.com for further clarification so we can add the relevant information to this document.

### References

Tennant et al., 2018; 10.1016/j.celrep.2018.01.005
Tennant et al., 2022; 10.1016/j.cub.2022.08.050
Clark and Nolan, 2024; 10.7554/eLife.89356.3
